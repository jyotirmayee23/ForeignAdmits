{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install pypdf\n",
    "# !pip install faiss-cpu\n",
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5304/2122603636.py:17: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import ChatBedrock``.\n",
      "  llm = BedrockChat(\n"
     ]
    }
   ],
   "source": [
    "bedrock_runtime = boto3.client( \n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "        model_id=\"amazon.titan-embed-text-v1\",\n",
    "        client=bedrock_runtime,\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "        vectorstore_cls=FAISS,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=bedrock_runtime,\n",
    "    region_name=\"us-east-1\",\n",
    "    model_kwargs={\"temperature\": 0.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'model_id': 'anthropic.claude-3-haiku-20240307-v1:0', 'usage': {'prompt_tokens': 9, 'completion_tokens': 12, 'total_tokens': 21}}, id='run-9c38ec86-fa21-42d6-a3c2-e1bd61291621-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 238 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"Statement of Purpose Development Questionnaire.docx (1).pdf\")\n",
    "index_from_loader = index_creator.from_loaders([loader])\n",
    "index_from_loader.vectorstore.save_local(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.load_local(\"/tmp\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever=faiss_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return the the documents related to question\",\n",
    "    )\n",
    "\n",
    "tools=[retriever_tool]\n",
    "retrieve=ToolNode([retriever_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    region_name = \"us-east-1\"\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    region_name = \"us-east-1\"\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Sample question bank\n",
    "question_bank = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you explain the difference between supervised and unsupervised learning?\",\n",
    "    \"What is overfitting and how can you prevent it?\"\n",
    "]\n",
    "\n",
    "# Agent state definition\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    question_index: int\n",
    "    validated_answers: list\n",
    "    current_question: str\n",
    "    user_answer: str\n",
    "    answer_validated: bool\n",
    "\n",
    "# 1. Greet the user\n",
    "def greet_user(state: AgentState) -> AgentState:\n",
    "    print(\"🤖 Hi there! I'm your interview assistant. Let's get started.\")\n",
    "    return state\n",
    "\n",
    "# 2. Wait for user's greeting\n",
    "def wait_for_user_greeting(state: AgentState) -> AgentState:\n",
    "    greeting = input(\"You: \").strip().lower()\n",
    "    if any(word in greeting for word in [\"hi\", \"hello\", \"hey\", \"good\", \"ready\"]):\n",
    "        return state\n",
    "    else:\n",
    "        print(\"🤖 Just say hi when you're ready.\")\n",
    "        return wait_for_user_greeting(state)\n",
    "\n",
    "# 3. Ask the current question from the question bank\n",
    "def ask_question(state: AgentState) -> AgentState:\n",
    "    index = state.get(\"question_index\", 0)\n",
    "    question = question_bank[index]\n",
    "    print(f\"🤖 Question {index + 1}: {question}\")\n",
    "    state[\"current_question\"] = question\n",
    "    return state\n",
    "\n",
    "# 4. Get the user's answer (CLI input)\n",
    "def get_user_answer(state: AgentState) -> AgentState:\n",
    "    state[\"user_answer\"] = input(\"Your answer: \").strip()\n",
    "    return state\n",
    "\n",
    "# 5. Validate the answer using Claude and retriever\n",
    "def validate_answer(state: AgentState, tools) -> AgentState:\n",
    "    # Use the retriever tool to fetch relevant docs\n",
    "    retriever_tool = tools[0]\n",
    "    retrieved_docs = retriever_tool.invoke({\"question\": state[\"current_question\"]})\n",
    "\n",
    "    # Call Claude with the answer and the retrieved context\n",
    "    context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    prompt = f\"\"\"Here is a user's answer to a question.\n",
    "\n",
    "    Question: {state[\"current_question\"]}\n",
    "    Answer: {state[\"user_answer\"]}\n",
    "\n",
    "    Use the following context to validate the answer:\n",
    "    {context}\n",
    "\n",
    "    Is the answer correct and complete? Reply 'yes' or 'no' and explain why.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"🤖 Validation result:\", response.content)\n",
    "\n",
    "    if \"yes\" in response.content.lower():\n",
    "        state[\"answer_validated\"] = True\n",
    "    else:\n",
    "        state[\"answer_validated\"] = False\n",
    "    return state\n",
    "\n",
    "# 6. Ask follow-up question\n",
    "def ask_follow_up(state: AgentState) -> AgentState:\n",
    "    follow_up = f\"Can you clarify or expand more on this: {state['current_question']}?\"\n",
    "    print(f\"🤖 Follow-up: {follow_up}\")\n",
    "    return state\n",
    "\n",
    "# 7. Save validated answer and advance to next question\n",
    "def save_valid_answer(state: AgentState) -> AgentState:\n",
    "    state[\"validated_answers\"].append({\n",
    "        \"question\": state[\"current_question\"],\n",
    "        \"answer\": state[\"user_answer\"]\n",
    "    })\n",
    "    state[\"question_index\"] += 1\n",
    "    return state\n",
    "\n",
    "def wait_for_answer(state: AgentState) -> AgentState:\n",
    "    state[\"user_answer\"] = input(\"Your answer: \").strip()\n",
    "    return state\n",
    "\n",
    "\n",
    "# 8. End the interview\n",
    "def end_interview(state: AgentState) -> AgentState:\n",
    "    print(\"🤖 That's all for now. Thanks for your answers!\")\n",
    "    print(\"📄 Summary of your validated answers:\")\n",
    "    for i, entry in enumerate(state[\"validated_answers\"], 1):\n",
    "        print(f\"{i}. {entry['question']} ➡ {entry['answer']}\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import StateGraph, ToolNode, Node\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Create your graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"start\", greet_user)\n",
    "graph.add_node(\"wait_for_greeting\", wait_for_user_greeting)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"wait_for_answer\", wait_for_answer)\n",
    "# graph.add_node(\"validate_answer\", ToolNode([retriever_tool]))\n",
    "graph.add_node(\"validate_answer\", validate_answer)\n",
    "graph.add_node(\"follow_up\", ask_follow_up)\n",
    "graph.add_node(\"save_answer\", save_valid_answer)\n",
    "graph.add_node(\"end\", end_interview)\n",
    "\n",
    "# Define the flow\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"wait_for_greeting\")\n",
    "graph.add_edge(\"wait_for_greeting\", \"ask_question\")\n",
    "graph.add_edge(\"ask_question\", \"wait_for_answer\")\n",
    "graph.add_edge(\"wait_for_answer\", \"validate_answer\")\n",
    "\n",
    "# Decision logic after validation\n",
    "def validation_router(state: AgentState) -> str:\n",
    "    if state.get(\"answer_validated\", False):\n",
    "        return \"save_answer\"\n",
    "    else:\n",
    "        return \"follow_up\"\n",
    "\n",
    "graph.add_conditional_edges(\"validate_answer\", validation_router)\n",
    "\n",
    "# Follow-up response path\n",
    "graph.add_edge(\"follow_up\", \"wait_for_answer\")\n",
    "\n",
    "# Valid path: save then continue or finish\n",
    "def next_step(state: AgentState) -> str:\n",
    "    if state[\"question_index\"] + 1 < len(question_bank):\n",
    "        return \"ask_question\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "graph.add_conditional_edges(\"save_answer\", next_step)\n",
    "\n",
    "# Compile graph\n",
    "interview_graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "app=graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"issue\")\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Hi there! I'm your interview assistant. Let's get started.\n"
     ]
    }
   ],
   "source": [
    "# Example initial state\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"question_index\": 0,\n",
    "    \"validated_answers\": [],\n",
    "    \"current_question\": \"What's your name?\",\n",
    "    \"user_answer\": \"\",\n",
    "    \"answer_validated\": False\n",
    "}\n",
    "\n",
    "# Invoke the interview graph correctly without passing state as a keyword\n",
    "interview_graph.invoke(input=initial_state)  # Pass the state as input directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
